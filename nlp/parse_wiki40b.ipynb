{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parse_wiki40b.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wiki-40Bデータセットを構造化する\n"
      ],
      "metadata": {
        "id": "znaL_6r6pd4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ライブラリのインストール"
      ],
      "metadata": {
        "id": "N6eE6_7ms7UJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CBwLSDbnpbEb"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 設定"
      ],
      "metadata": {
        "id": "y0KJetDHs-s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LANGUAGE = \"ja\"\n",
        "WIKI40B_VERSION = \"1.3.0\""
      ],
      "metadata": {
        "id": "IYgzYLGapo77"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wiki-40Bデータセットのダウンロード\n",
        "\n",
        "ダウンロードしたデータセットをファイルに保存する。"
      ],
      "metadata": {
        "id": "drq94HUxs2Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "ds, ds_info = tfds.load(name=f\"wiki40b/{LANGUAGE}\", \n",
        "          shuffle_files=False,\n",
        "          with_info=True)\n",
        "\n",
        "print(ds_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiIVUxT4p0Wu",
        "outputId": "fe4a27da-44b3-4225-c700-33b531d904cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='wiki40b',\n",
            "    version=1.3.0,\n",
            "    description='Clean-up text for 40+ Wikipedia languages editions of pages\n",
            "correspond to entities. The datasets have train/dev/test splits per language.\n",
            "The dataset is cleaned up by page filtering to remove disambiguation pages,\n",
            "redirect pages, deleted pages, and non-entity pages. Each example contains the\n",
            "wikidata id of the entity, and the full Wikipedia article after page processing\n",
            "that removes non-content sections and structured objects. The language models\n",
            "trained on this corpus - including 41 monolingual models, and 2 multilingual\n",
            "models - can be found at https://tfhub.dev/google/collections/wiki40b-lm/1.',\n",
            "    homepage='https://research.google/pubs/pub49029/',\n",
            "    features=FeaturesDict({\n",
            "        'text': Text(shape=(), dtype=tf.string),\n",
            "        'version_id': Text(shape=(), dtype=tf.string),\n",
            "        'wikidata_id': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    total_num_examples=828236,\n",
            "    splits={\n",
            "        'test': 41268,\n",
            "        'train': 745392,\n",
            "        'validation': 41576,\n",
            "    },\n",
            "    supervised_keys=None,\n",
            "    citation=\"\"\"@inproceedings{49029,\n",
            "    title = {Wiki-40B: Multilingual Language Model Dataset},\n",
            "    author = {Mandy Guo and Zihang Dai and Denny Vrandecic and Rami Al-Rfou},\n",
            "    year = {2020},\n",
            "    booktitle\t= {LREC 2020}\n",
            "    }\"\"\",\n",
            "    redistribution_info=license: \"This work is licensed under the Creative Commons Attribution-ShareAlike\\n3.0 Unported License. To view a copy of this license, visit\\nhttp://creativecommons.org/licenses/by-sa/3.0/ or send a letter to\\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\",\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 表示してみる\n",
        "\n",
        "for i, datum in enumerate(ds[\"test\"].as_numpy_iterator()):\n",
        "    text = datum[\"text\"].decode(\"utf-8\")\n",
        "    version_id = datum[\"version_id\"].decode(\"utf-8\")\n",
        "    wikidata_id = datum[\"wikidata_id\"].decode(\"utf-8\")\n",
        "\n",
        "    print(f\"{i}. version_id: {version_id}, wikidata_id: {wikidata_id}, text: {text}\")\n",
        "    print(\"------------------------------------------------\")\n",
        "    print()\n",
        "\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLxw50hHyGIy",
        "outputId": "d4eaf2e5-2340-4880-9bf0-0c59aa447f56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. version_id: 1848243370795951995, wikidata_id: Q11331136, text: \n",
            "_START_ARTICLE_\n",
            "ビートたけしの教科書に載らない日本人の謎\n",
            "_START_SECTION_\n",
            "概要\n",
            "_START_PARAGRAPH_\n",
            "「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\n",
            "------------------------------------------------\n",
            "\n",
            "1. version_id: 13993402720669107168, wikidata_id: Q17218581, text: \n",
            "_START_ARTICLE_\n",
            "ゲオネットワークス\n",
            "_START_SECTION_\n",
            "概要\n",
            "_START_PARAGRAPH_\n",
            "ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。_NEWLINE_2005年に株式会社ライブドアから分割されて設立。_NEWLINE_かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。_NEWLINE_「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。_NEWLINE_オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。\n",
            "_START_SECTION_\n",
            "ラジオCM\n",
            "_START_PARAGRAPH_\n",
            "2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。\n",
            "------------------------------------------------\n",
            "\n",
            "2. version_id: 2064341086175500784, wikidata_id: Q11667331, text: \n",
            "_START_ARTICLE_\n",
            "香川県信用農業協同組合連合会\n",
            "_START_SECTION_\n",
            "概要\n",
            "_START_PARAGRAPH_\n",
            "香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。\n",
            "------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import tensorflow_datasets as tfds\n",
        "import gzip\n",
        "\n",
        "for split_name in [\"test\", \"validation\", \"train\"]:\n",
        "    ds, ds_info = tfds.load(f\"wiki40b/{LANGUAGE}:{WIKI40B_VERSION}\", \n",
        "        shuffle_files=False,\n",
        "        with_info=True)\n",
        "\n",
        "    with gzip.open(f\"wiki40b_{LANGUAGE}_{WIKI40B_VERSION}_{split_name}.jsonl.gz\", \"wt\", encoding=\"utf-8\") as f_out:\n",
        "        for datum in tqdm(ds[split_name].as_numpy_iterator(), total=ds_info.splits[split_name].num_examples):\n",
        "            text = datum[\"text\"].decode(\"utf-8\")\n",
        "            version_id = datum[\"version_id\"].decode(\"utf-8\")\n",
        "            wikidata_id = datum[\"wikidata_id\"].decode(\"utf-8\")\n",
        "\n",
        "            f_out.write(json.dumps({\n",
        "                \"version_id\": version_id, \n",
        "                \"wikidata_id\": wikidata_id, \n",
        "                \"text\": text\n",
        "                }, ensure_ascii=True))\n",
        "            f_out.write(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYRjA6usvs4",
        "outputId": "bb29ba44-55e6-4703-8c01-7b9dbfb316ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41268/41268 [01:54<00:00, 361.28it/s]\n",
            "100%|██████████| 41576/41576 [01:53<00:00, 367.67it/s]\n",
            "100%|██████████| 745392/745392 [35:14<00:00, 352.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wiki-40Bデータセットの構造解析\n",
        "\n",
        "Wiki-40Bデータセットからタイトル、セクション、パラグラフの情報を抽出する。  \n",
        "\n",
        "注意:次のデータは読み捨てている。\n",
        "\n",
        "- タイトルやセクション名に改行文字が入ってる場合、その文書全体を無視する。\n",
        "- セクション名だけでパラグラフがない場合、そのセクションを無視する。\n",
        "- その他、コード中のアサーションに違反している場合、その文書全体を無視する。"
      ],
      "metadata": {
        "id": "8J28mlRqtqck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import gzip\n",
        "import enum\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def normalize_ja_text(text):\n",
        "    # 必要に応じて正規化処理を施す。この例では何もしない。\n",
        "    return text\n",
        "\n",
        "\n",
        "class ParseMode(enum.Enum):\n",
        "    INIT = enum.auto()\n",
        "    IN_TITLE = enum.auto()\n",
        "    IN_SECTION = enum.auto()\n",
        "    IN_PARAGRAPH = enum.auto()\n",
        "    EXPECT_SECTION = enum.auto()\n",
        "    EXPECT_PARAGRAPH = enum.auto()\n",
        "\n",
        "\n",
        "def extract_content(text, normalizer):\n",
        "    title = None\n",
        "    sections = []\n",
        "\n",
        "    def normalize_line(line):\n",
        "        assert \"\\n\" not in line\n",
        "        lines = line.split(\"_NEWLINE_\")\n",
        "        line = normalizer(\"_NEWLINE_\").join([normalizer(l.strip()) for l in lines])\n",
        "        return line\n",
        "\n",
        "    mode = ParseMode.INIT\n",
        "\n",
        "    for line in text.split(\"\\n\")[1:]:\n",
        "        line = line.strip()\n",
        "        if mode == ParseMode.INIT:\n",
        "            assert line == \"_START_ARTICLE_\", line\n",
        "            mode = ParseMode.IN_TITLE\n",
        "        elif mode == ParseMode.IN_TITLE:\n",
        "            title = normalize_line(line)\n",
        "            mode = ParseMode.EXPECT_SECTION\n",
        "        elif mode == ParseMode.IN_SECTION:\n",
        "            sections.append({\"name\": normalize_line(line), \"paragraph\": None})\n",
        "            mode = ParseMode.EXPECT_PARAGRAPH\n",
        "        elif mode == ParseMode.IN_PARAGRAPH:\n",
        "            assert len(sections) > 0, line\n",
        "            last_section = sections[-1]\n",
        "            assert last_section[\"paragraph\"] is None, line\n",
        "            last_section[\"paragraph\"] = normalize_line(line)\n",
        "            mode = ParseMode.EXPECT_SECTION\n",
        "        elif mode == ParseMode.EXPECT_SECTION:\n",
        "            assert line == \"_START_SECTION_\" or line == \"_START_PARAGRAPH_\", line\n",
        "            if line == \"_START_SECTION_\":\n",
        "                mode = ParseMode.IN_SECTION\n",
        "            elif line == \"_START_PARAGRAPH_\":\n",
        "                sections.append({\"name\": None, \"paragraph\": None})\n",
        "                mode = ParseMode.IN_PARAGRAPH\n",
        "        elif mode == ParseMode.EXPECT_PARAGRAPH:\n",
        "            assert line == \"_START_PARAGRAPH_\", line\n",
        "            mode = ParseMode.IN_PARAGRAPH\n",
        "        \n",
        "    for section in sections:\n",
        "        assert section[\"paragraph\"] is not None, text\n",
        "\n",
        "    sections = [section for section in sections if len(section[\"paragraph\"]) > 0]  # パラグラフのないセクションは無視する。\n",
        "\n",
        "    return title, sections\n",
        "\n",
        "\n",
        "def parse_wiki40b(language, wiki40b_version, split_name, normalizer):\n",
        "    with gzip.open(f\"wiki40b_{language}_{wiki40b_version}_{split_name}.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f_in, \\\n",
        "        gzip.open(f\"wiki40b_{language}_{wiki40b_version}_{split_name}.parsed.jsonl.gz\", \"wt\", encoding=\"utf-8\") as f_parsed:\n",
        "\n",
        "        for line in f_in:\n",
        "            page_data = json.loads(line)\n",
        "\n",
        "            version_id = page_data[\"version_id\"]\n",
        "            wikidata_id = page_data[\"wikidata_id\"]\n",
        "            text = page_data[\"text\"]\n",
        "            try:\n",
        "                title, sections = extract_content(text, normalizer)\n",
        "            except AssertionError as err:\n",
        "                # アサーションエラーがあった場合は処理をスキップする。\n",
        "                print(f\"WARN: {err} in {wikidata_id}\")\n",
        "                # raise err\n",
        "                continue\n",
        "\n",
        "            assert title is not None\n",
        "            normalized_text = normalizer(\"_START_ARTICLE_\") + \"\\n\"\n",
        "            normalized_text += title + \"\\n\"\n",
        "\n",
        "            for section in sections:\n",
        "                name = section[\"name\"]\n",
        "                paragraph = section[\"paragraph\"]\n",
        "                if name is not None:\n",
        "                    normalized_text += normalizer(\"_START_SECTION_\") + \"\\n\"\n",
        "                    normalized_text += name + \"\\n\"\n",
        "\n",
        "                assert paragraph is not None\n",
        "                normalized_text += normalizer(\"_START_PARAGRAPH_\") + \"\\n\"\n",
        "                normalized_text += paragraph + \"\\n\"\n",
        "\n",
        "            page_data[\"normalized_title\"] = title\n",
        "            page_data[\"normalized_sections\"] = sections\n",
        "            page_data[\"normalized_text\"] = normalized_text\n",
        "            \n",
        "            f_parsed.write(json.dumps(page_data, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "PYopLL-jtdzW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_wiki40b(LANGUAGE, WIKI40B_VERSION, \"test\",  normalize_ja_text)\n",
        "parse_wiki40b(LANGUAGE, WIKI40B_VERSION, \"validation\", normalize_ja_text)\n",
        "parse_wiki40b(LANGUAGE, WIKI40B_VERSION, \"train\", normalize_ja_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QThY2yx8vvVn",
        "outputId": "d6ed7561-fbab-42e5-fa49-68685481005f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARN:  in Q1916324\n",
            "WARN:  in Q56354326\n",
            "WARN:  in Q5015732\n",
            "WARN:  in Q2005236\n",
            "WARN:  in Q1158767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 構造解析した結果の表示"
      ],
      "metadata": {
        "id": "f9OOCVZ3wm4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import gzip\n",
        "\n",
        "\n",
        "with gzip.open(f\"wiki40b_{LANGUAGE}_{WIKI40B_VERSION}_test.parsed.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
        "    for i, json_line in enumerate(f):\n",
        "        article = json.loads(json_line)\n",
        "        print(json.dumps(article, ensure_ascii=False, indent=4))\n",
        "        print(\"------------------------------------------------\")\n",
        "        print()\n",
        "        \n",
        "        if i >= 2:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYHxox9mwqnI",
        "outputId": "828678bb-ada3-4930-e0f0-135e72d6e3f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"version_id\": \"1848243370795951995\",\n",
            "    \"wikidata_id\": \"Q11331136\",\n",
            "    \"text\": \"\\n_START_ARTICLE_\\nビートたけしの教科書に載らない日本人の謎\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\",\n",
            "    \"normalized_title\": \"ビートたけしの教科書に載らない日本人の謎\",\n",
            "    \"normalized_sections\": [\n",
            "        {\n",
            "            \"name\": \"概要\",\n",
            "            \"paragraph\": \"「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\"\n",
            "        }\n",
            "    ],\n",
            "    \"normalized_text\": \"_START_ARTICLE_\\nビートたけしの教科書に載らない日本人の謎\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\\n\"\n",
            "}\n",
            "------------------------------------------------\n",
            "\n",
            "{\n",
            "    \"version_id\": \"13993402720669107168\",\n",
            "    \"wikidata_id\": \"Q17218581\",\n",
            "    \"text\": \"\\n_START_ARTICLE_\\nゲオネットワークス\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\nライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。_NEWLINE_2005年に株式会社ライブドアから分割されて設立。_NEWLINE_かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。_NEWLINE_「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。_NEWLINE_オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。\\n_START_SECTION_\\nラジオCM\\n_START_PARAGRAPH_\\n2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。\",\n",
            "    \"normalized_title\": \"ゲオネットワークス\",\n",
            "    \"normalized_sections\": [\n",
            "        {\n",
            "            \"name\": \"概要\",\n",
            "            \"paragraph\": \"ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。_NEWLINE_2005年に株式会社ライブドアから分割されて設立。_NEWLINE_かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。_NEWLINE_「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。_NEWLINE_オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"ラジオCM\",\n",
            "            \"paragraph\": \"2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。\"\n",
            "        }\n",
            "    ],\n",
            "    \"normalized_text\": \"_START_ARTICLE_\\nゲオネットワークス\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\nライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。_NEWLINE_2005年に株式会社ライブドアから分割されて設立。_NEWLINE_かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。_NEWLINE_「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。_NEWLINE_オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。\\n_START_SECTION_\\nラジオCM\\n_START_PARAGRAPH_\\n2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。\\n\"\n",
            "}\n",
            "------------------------------------------------\n",
            "\n",
            "{\n",
            "    \"version_id\": \"2064341086175500784\",\n",
            "    \"wikidata_id\": \"Q11667331\",\n",
            "    \"text\": \"\\n_START_ARTICLE_\\n香川県信用農業協同組合連合会\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。\",\n",
            "    \"normalized_title\": \"香川県信用農業協同組合連合会\",\n",
            "    \"normalized_sections\": [\n",
            "        {\n",
            "            \"name\": \"概要\",\n",
            "            \"paragraph\": \"香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。\"\n",
            "        }\n",
            "    ],\n",
            "    \"normalized_text\": \"_START_ARTICLE_\\n香川県信用農業協同組合連合会\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。\\n\"\n",
            "}\n",
            "------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dfIHXPmg8Acs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}